#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Diffusion Policy æ¼”ç¤ºç¨‹åº
å±•ç¤ºï¼š
1. æ»šåŠ¨æ—¶åŸŸä¼˜åŒ– (Receding Horizon Control)
2. æ˜ç¡®çš„åŠ å™ªå’Œå»å™ªè¿‡ç¨‹
3. å¯é…ç½®çš„æ‰©æ•£å‚æ•°
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from algo.DiffusionPolicy.diffusion_mlp import DiffusionPolicy_MLP
from envs.rl_reach_env import RLReachEnv
import argparse

def demonstrate_diffusion_features():
    """æ¼”ç¤ºDiffusion Policyçš„æ ¸å¿ƒç‰¹æ€§"""
    print("=" * 60)
    print("ğŸ¤– Diffusion Policy å¢å¼ºåŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºç¯å¢ƒ
    env = RLReachEnv(is_render=False, is_good_view=False)
    state_dim = 6
    action_dim = 3
    action_bound = 1.0
    
    # åˆ›å»ºä¸åŒé…ç½®çš„Diffusion Policy agents
    configs = [
        {
            "name": "å¿«é€Ÿæ¨ç†é…ç½®", 
            "num_diffusion_steps": 20,
            "num_inference_steps": 5,
            "horizon_steps": 4,
            "action_horizon": 2,
            "beta_schedule": "linear"
        },
        {
            "name": "æ ‡å‡†é…ç½®", 
            "num_diffusion_steps": 50,
            "num_inference_steps": 25,
            "horizon_steps": 8,
            "action_horizon": 4,
            "beta_schedule": "squaredcos_cap_v2"
        },
        {
            "name": "é«˜è´¨é‡é…ç½®",
            "num_diffusion_steps": 100,
            "num_inference_steps": 50,
            "horizon_steps": 16,
            "action_horizon": 8,
            "beta_schedule": "squaredcos_cap_v2"
        }
    ]
    
    for i, config in enumerate(configs):
        print(f"\nğŸ“Š é…ç½® {i+1}: {config['name']}")
        print("-" * 40)
        for key, value in config.items():
            if key != "name":
                print(f"  {key}: {value}")
        
        # åˆ›å»ºagent
        agent = DiffusionPolicy_MLP(
            state_dim=state_dim,
            action_dim=action_dim,
            action_bound=action_bound,
            hidden_dim=128,
            actor_lr=3e-4,
            num_diffusion_steps=config["num_diffusion_steps"],
            num_inference_steps=config["num_inference_steps"],
            beta_schedule=config["beta_schedule"],
            prediction_type="epsilon",
            ema_decay=0.995,
            clip_sample=True,
            network_type="mlp",
            horizon_steps=config["horizon_steps"],
            action_horizon=config["action_horizon"],
            device="cpu"
        )
        
        # æ¼”ç¤ºæ»šåŠ¨æ—¶åŸŸä¼˜åŒ–
        print(f"\nğŸ¯ æ»šåŠ¨æ—¶åŸŸä¼˜åŒ–æ¼”ç¤º:")
        state = env.reset()
        print(f"  åˆå§‹çŠ¶æ€: {state[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ªå…ƒç´ 
        
        # ç”ŸæˆåŠ¨ä½œåºåˆ—
        action_sequence = agent._generate_action_sequence(state, use_ema=False)
        print(f"  ç”ŸæˆåŠ¨ä½œåºåˆ—é•¿åº¦: {len(action_sequence)}")
        print(f"  é¢„æµ‹æ—¶åŸŸ: {config['horizon_steps']} æ­¥")
        print(f"  æ‰§è¡Œæ—¶åŸŸ: {config['action_horizon']} æ­¥")
        
        # æ¨¡æ‹Ÿæ‰§è¡Œå¤šæ­¥
        print(f"\nğŸ“ˆ æ‰§è¡Œåºåˆ—:")
        for step in range(min(3, config['action_horizon'])):
            action = agent.take_action(state, replan=False)  # ä¸é‡æ–°è§„åˆ’ï¼Œä½¿ç”¨åºåˆ—
            print(f"  æ­¥éª¤ {step+1}: åŠ¨ä½œ = [{action[0]:.3f}, {action[1]:.3f}, {action[2]:.3f}]")
            # è¿™é‡Œå¯ä»¥æ‰§è¡Œ state, _, _, _ = env.step(action) ä½†ä¸ºäº†æ¼”ç¤ºè·³è¿‡
        
        print(f"  ğŸ’¡ å‰©ä½™åºåˆ—é•¿åº¦: {len(agent.current_action_sequence) if agent.current_action_sequence is not None else 0}")
        
    # æ¼”ç¤ºå™ªå£°è°ƒåº¦
    print(f"\nğŸ”„ å™ªå£°è°ƒåº¦å¯¹æ¯”æ¼”ç¤º:")
    print("-" * 40)
    
    schedules = ["linear", "squaredcos_cap_v2"]
    steps = 20
    
    for schedule in schedules:
        from algo.DiffusionPolicy.diffusion_utils import NoiseScheduler
        scheduler = NoiseScheduler(
            num_diffusion_steps=steps,
            beta_schedule=schedule,
            clip_sample=True,
            prediction_type="epsilon"
        )
        
        print(f"\nğŸ“‰ {schedule} è°ƒåº¦:")
        print(f"  beta èŒƒå›´: [{scheduler.betas[0]:.6f}, {scheduler.betas[-1]:.6f}]")
        print(f"  alpha èŒƒå›´: [{scheduler.alphas[0]:.6f}, {scheduler.alphas[-1]:.6f}]")
        
        # å±•ç¤ºå‡ ä¸ªå…³é”®æ—¶é—´æ­¥çš„å™ªå£°æ°´å¹³
        key_steps = [0, steps//4, steps//2, 3*steps//4, steps-1]
        print(f"  å…³é”®æ—¶é—´æ­¥çš„å™ªå£°æ°´å¹³:")
        for t in key_steps:
            beta = scheduler.betas[t]
            alpha = scheduler.alphas[t]
            print(f"    t={t:2d}: beta={beta:.4f}, alpha={alpha:.4f}")

def demonstrate_training_process():
    """æ¼”ç¤ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ å™ªå’Œå»å™ª"""
    print(f"\nğŸ“ è®­ç»ƒè¿‡ç¨‹æ¼”ç¤º:")
    print("-" * 40)
    
    # åˆ›å»ºç®€å•çš„agent
    agent = DiffusionPolicy_MLP(
        state_dim=6, action_dim=3, action_bound=1.0,
        num_diffusion_steps=10, num_inference_steps=5,
        horizon_steps=4, action_horizon=2, device="cpu"
    )
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    states = np.random.randn(4, 6)  # 4ä¸ªçŠ¶æ€
    actions = np.random.randn(4, 3) * 0.5  # 4ä¸ªåŠ¨ä½œ
    
    print(f"ğŸ“Š æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®:")
    print(f"  çŠ¶æ€æ‰¹æ¬¡å¤§å°: {states.shape}")
    print(f"  åŠ¨ä½œæ‰¹æ¬¡å¤§å°: {actions.shape}")
    
    # å±•ç¤ºåŠ å™ªè¿‡ç¨‹
    print(f"\nâ• åŠ å™ªè¿‡ç¨‹æ¼”ç¤º:")
    states_tensor = torch.tensor(states, dtype=torch.float32)
    actions_tensor = torch.tensor(actions, dtype=torch.float32)
    
    # è½¬æ¢ä¸ºåŠ¨ä½œåºåˆ—
    batch_size = actions_tensor.shape[0]
    action_sequences = actions_tensor.unsqueeze(1).repeat(1, agent.horizon_steps, 1)
    action_sequences = action_sequences.reshape(batch_size, -1)
    
    print(f"  åŸå§‹åŠ¨ä½œ: {actions[0]}")
    print(f"  åŠ¨ä½œåºåˆ—å½¢çŠ¶: {action_sequences.shape}")
    
    # éšæœºæ—¶é—´æ­¥
    timesteps = torch.randint(0, agent.noise_scheduler.num_diffusion_steps, (batch_size,))
    print(f"  éšæœºæ—¶é—´æ­¥: {timesteps.numpy()}")
    
    # åŠ å™ª
    noise = torch.randn_like(action_sequences)
    noisy_actions = agent.noise_scheduler.add_noise(action_sequences, noise, timesteps)
    
    print(f"  å™ªå£°æ ·æœ¬: {noise[0, :3].numpy()}")  # åªæ˜¾ç¤ºå‰3ä¸ª
    print(f"  åŠ å™ªååŠ¨ä½œ: {noisy_actions[0, :3].numpy()}")  # åªæ˜¾ç¤ºå‰3ä¸ª
    
    print(f"\nâœ… æ¼”ç¤ºå®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Diffusion Policy å¢å¼ºåŠŸèƒ½æ¼”ç¤º")
    parser.add_argument("--demo", type=str, choices=["features", "training", "all"], 
                       default="all", help="é€‰æ‹©æ¼”ç¤ºå†…å®¹")
    args = parser.parse_args()
    
    if args.demo in ["features", "all"]:
        demonstrate_diffusion_features()
    
    if args.demo in ["training", "all"]:
        demonstrate_training_process()

if __name__ == "__main__":
    main() 